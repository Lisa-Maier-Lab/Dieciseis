---
title: "Process 16S data using DADA2"
format: html
---

# Aim

Jacobo de la Cuesta-Zuluaga. February 2025.

This notebook illustrates how to process raw 16S rRNA gene amplicons using `DADA2`,
from the cleaning of raw sequences, the identification of amplicon sequence variants
(ASVs) and their general taxonomic classification.

If you are processing sequences from a synthetic community such as Com20, you
will need to run the second notebook `02_Synth_Com_taxonomy` which covers a more 
specific taxonomic classification and the collapsing the abundance of ASVs by species.

For tutorials and explanations of the steps, see:
https://benjjneb.github.io/dada2/tutorial_1_8.html 
https://alexiscarter.github.io/metab/Dada_script_EN.html

# Notes and updates

## Februrary 2025

In this updated version of the notebook, I have made changes to reflect our change
of sequencing provider. The lab is now using the services of a sequencing facility
that uses the V3-V4 region of the 16S rRNA gene, and a newer sequencing platform
that changes the way the error profiles are stored in the `fastq` files. This
so-called "binned" error profiles alter the way `DADA2` deals with the sequences
and thus require some adjustments on the code.

I have added a few steps where you, the user, need to specify what kind of 
sequences are using, that is, the new binned errors or the older linear errors. 
This way, the notebook will be usable regardless of the sequencing you have. 
*Note* that if you have reads using two different profiles, you will need to
process them separately!

Other changes: 

* I added a filtering step to remove ASVs that have no classification at the 
phylum level, you can find an explanation below.

* I moved the generation of the ASV phylogeny to an optional section at the end
of the notebook.


# Libraries

The following libraries are required for the present workflow

```{r}
library(tidyverse)
library(dada2)
library(DECIPHER)
library(ape)
library(Biostrings)
library(digest)
library(phangorn)
library(conflicted)
```


```{r}
# Solve conflicts with certain function names
conflict_prefer("filter", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("rename", "dplyr")
conflict_prefer("setdiff", "base")
```

```{r}
# Load helper functions
source("../bin/Helper_functions.R")
```


# Paths
You need to make sure that the pathways to the directories with your sequence
files, the references and the samples file are correct. 

The present notebook is filled by default with the paths pointing to a few 
test files used to make sure that the workflow runs correctly.

*Note* that at several points throughout the notebook you will be prompted to
respond whether the paths are correct so you can continue. For example, here
you need to change the path saved in the `base_dir` variable.

```{r}
# TODO make sure the directories are correct
Check_point("Are the base and out directories filled?")

base_dir = "../test"

# Output directory
out_dir = file.path(base_dir, "dieciseis_out")
```

```{r}
# TODO make sure the path to the reference files is correct
Check_point("Is the path to the reference files correct?")

# Reference dbs
ref_dir = "../reference_files"

gtdb_tax = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna.gz")
gtdb_species = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.addSpecies.fna.gz")
```

*Note* that I had to manually change the GTDB reference files. This is already
fixed but it is here just as reference. You don't need to modify these files
further.

The files were originally obtained on June 2022 from:
https://scilifelab.figshare.com/articles/dataset/SBDI_Sativa_curated_16S_GTDB_database/14869077

They have a double domain entry because the authors of these files require them 
in such way for their pipeline. However, this leads to problems when used 
directly on dada2.

To fix, I used the following commands:

```
zcat gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fna.gz | sed -E 's~>Bacteria;|>Archaea;~>~' | sed '/>.*/s/$/;/' > gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
gzip gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
```
```{r}
# Filtered sequences
# In most cases you don't need to change this
filt_seqs_dir = tempdir()
```

# Sequencing facility

This notebook includes two sets of test files:

1. Samples from the V4 region of the 16S rRNA gene sequenced with Illumina MiSeq
using linear error profiles. This is the kind of samples obtained from `NCCT`

2. Samples corresponding to the V3-V4 region of the 16S rRNA gene sequenced
with Illumina MiSeq using binned error profiles. This is the kind of samples
obtained from `GeneWiz`

Here, you need to select what kind of samples you have
```{r}
# TODO Sequencing facility
Check_point("Did you select the appropriate sequencing facility?")

# options: NCCT or GeneWiz
sequencing_facility = "GeneWiz" 
```

# Samples file
You need to create a table with three columns, named `samplename`, `forward` and
`reverse`. They should contain the name of the sample and their corresponding
*forward* and *reverse* paths. Note that this should be the absolute (complete)
path of the file.

```{r}
if(sequencing_facility == "NCCT"){
  warning("you selected NCCT as seq facility. Use the test_samplesfile.tsv in the example")
} else if(sequencing_facility == "GeneWiz") {
  warning("you selected GeneWiz as seq facility. Use the novaseq_samplesfile.tsv in the example")
}

# TODO make sure the path to the samples file is correct
Check_point("Is the path to the samples file correct?")

samples_file =  file.path(base_dir, "novaseq_samplesfile.tsv")

samples_df = samples_file %>% 
  read_tsv()
```

# Check forward and reverse files
Here you might need to modify the pattern used by the sequencing center to
label the forward and the reverse files. Check the names of your files and 
adjust accordingly.

You will then need to manually verify that for each file you have a forward
and a reverse file.

```{r}
# TODO make sure the patterns are correct
Check_point("If the correct file name pattern for Forward and Reverse reads added?")

forward_pattern = "_R1_001.fastq.gz"
reverse_pattern = "_R2_001.fastq.gz"

# List files in raw sequences dir
forward_reads = samples_df$forward
reverse_reads = samples_df$reverse

# Make sure the corresponding F and R files are present
forward_names = forward_reads %>% 
  basename() %>% 
  str_remove(forward_pattern) #

reverse_names = reverse_reads %>% 
  basename() %>% 
  str_remove(reverse_pattern)

# TODO make sure that the file names make sense and that all files are paired
setdiff(forward_names, reverse_names)
setdiff(reverse_names, forward_names)
```

# Execute DADA2
## Check quality of data
If you have a very large set of samples, you can select a fraction of them
to perform the visual QC, otherwise it can take a long time. 

*Note* that this subset is not recommended, change at your own risk.

```{r}
# Proportion of reads to randomly select
# To inspect all samples, use 1
QC_subset_proportion = 1
QC_subset_number = length(forward_reads) * QC_subset_proportion
```

```{r}
# Obtain quality plots of raw sequences for each of the files
Raw_QC_plotsF = sample(forward_reads, QC_subset_number) %>%
  map(., function(fastq) {plotQualityProfile(fastq)})

Raw_QC_plotsR = sample(reverse_reads, QC_subset_number) %>%
  map(., function(fastq) {plotQualityProfile(fastq)})
```

The following two chunks will plot the resulting QC graphs. For an explanation
of how to interpret these graphs, see the links to the tutorials at the top
of this notebook.

```{r fig.height=3, fig.width=5}
# Print forward plots
Raw_QC_plotsF_hline = map(Raw_QC_plotsF, function(x) x + 
                            geom_hline(yintercept = 30) + 
                            geom_vline(xintercept = 245))
Raw_QC_plotsF_hline
```

```{r fig.height=3, fig.width=5}
# Print Rreverse plots
Raw_QC_plotsR_hline = map(Raw_QC_plotsR, function(x) x + 
                            geom_hline(yintercept = 30) +
                            geom_vline(xintercept = 245))
Raw_QC_plotsR_hline
```


# Filter reads
The next step will perform the trimming of the reads based on sequence quality
and length. In addition, it removes the primer sequences from the reads. This
is based on *GeneWiz* sequencing results. If you're using sequences produced 
by *GeneWiz* and are satisfied by the quality values obtained above, you don't 
need to change the values here. 

The sequences obtained from *NCCT* use a sequencing chemistry in which the 
quality of the sequences decreases more towards the ends of the read. 
You will see a dip of the green line below 30 on the right side of the graph.
In that case, change the following parameter to something similar 
to `truncLen=c(225, 200)`.You can play with the value of the parameter 
`geom_vline(xintercept = 245)` in the chunks above to select an adequate value.

If using sequences from other centers or from public repositories, you will need
to check whether they contain the primer sequences, the expected amplicon length
and consider the quality of the reads.

```{r}
# Trim sequences based on sequencing facility
# This is selected based on the facility you specified above

if(sequencing_facility == "NCCT"){
  trim_left = c(23,24)
} else if(sequencing_facility == "GeneWiz") {
  trim_left = c(16,21)
} else {
  warning("Please select the sequencing facility")
}
```


```{r}
# Filtering
filtered_trimmed = filterAndTrim(fwd = forward_reads, 
              rev = reverse_reads, 
              filt = filt_seqs_dir, 
              filt.rev = filt_seqs_dir,
              trimLeft = trim_left, # Trim nucleotides corresponding to primers. To check align primers to raw seqs
              truncLen = c(240, 240), # Truncation position of F and R. Change if needed!
              maxEE = c(2,2), # Number of expected errors
              truncQ = 10, # Truncate reads at the first instance of a quality score less than
              rm.phix = TRUE, # filter matches against the phiX
              compress = TRUE, 
              verbose = TRUE, 
              multithread = FALSE,
              matchIDs = TRUE)
```

#  Learn errors

To generate the ASVs, DADA2 uses the sequence error profiles, using the
trimmed reads. As with the step dealing with raw sequence names above,
you need to check that the names from the filtered read files make sense.

```{r}
# List files in filtered sequences dir
forward_filt_reads = list.files(filt_seqs_dir, pattern = forward_pattern, full.names = TRUE)
reverse_filt_reads = list.files(filt_seqs_dir, pattern = reverse_pattern, full.names = TRUE)

# Make sure the corresponding F and R files are present
forward_names_filt = str_remove(basename(forward_filt_reads), forward_pattern)
reverse_names_filt = str_remove(basename(reverse_filt_reads), reverse_pattern)

setdiff(forward_names_filt, reverse_names_filt)
setdiff(reverse_names_filt, forward_names_filt)

# Add names to vectors
names(forward_filt_reads) = forward_names_filt
names(reverse_filt_reads) = reverse_names_filt
```

## Error profile function

First, you need to determine whether your error profile is binned or linear.
Raw sequencing files give an error value to each base, called a phred score, that
can go from 0 to 40 (using various symbols). A binned error profile groups values
into ranges.

Long story short, if you execute the following function and get something like
`2 11 25 37` it means that your errors are binned.

If you have samples showing something different, you will need to process them
separately.
```{r}
QualityBinValue(forward_reads, reverse_reads)
```


```{r}
# Type of error profile
# Linear error profile can be found in samples sequenced by NCCT
# Binned error profile is found in GeneWiz sequenced samples
# This is selected based on the facility you specified above

if(sequencing_facility == "NCCT"){
  error_function = loessErrfun
} else if(sequencing_facility == "GeneWiz") {
  binnedQs = c(2, 11, 25, 37) # Change if you have different values from the chunk above
  error_function = makeBinnedQualErrfun(binnedQs)
} else {
  warning("Please select the sequencing facility")
}
```

## Forward

Now you can execute the `learnErrors` function for forward and reverse reads.
You don't need to modify anything in the function call.

```{r}
# Set random seed
set.seed(2112)
```

```{r}
# Learn error rates
forward_errors = learnErrors(forward_filt_reads, 
                             nbases = 2e8, 
                             randomize = TRUE,
                             multithread = FALSE, 
                             verbose = TRUE, 
                             errorEstimationFunction = error_function)
```

```{r}
plotErrors(forward_errors, nominalQ=TRUE)
```

## Reverse
```{r}
reverse_errors = learnErrors(forward_filt_reads, 
                             nbases = 2e8, 
                             randomize = TRUE,
                             multithread = FALSE, 
                             verbose = TRUE,
                             errorEstimationFunction = error_function)
```


```{r}
plotErrors(reverse_errors, nominalQ = TRUE)
```
As with the QC plots, you need to interpret the error profile plots. It is
your responsibility to make sure everything looks as expected. 

For an explanation of this plot, please see the links to the tutorials above.

*Note* that the plot from the binned errors look different from that of linear
errors. For an explanation of the plot, see:
https://github.com/benjjneb/dada2/issues/1307#issuecomment-2521790524

In brief, the observed error rates per average Q score are scaled by the number 
of reads supporting them. That's why the largest circles are at 11, 25 and 37 in the
x axis


# Infer sequence variants

Afterwards, you can proceed to the generation of the ASVs.
You don't need to modify anything in the function call.

## Forward
```{r}
forward_derep = map(forward_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

forward_dada = map(forward_derep, 
                   function(derep_obj){dada(derep_obj, err = forward_errors,
                                            multithread = FALSE, verbose = TRUE)})
```

## Reverse
```{r}
reverse_derep = map(reverse_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

reverse_dada = map(reverse_derep, 
                   function(derep_obj){dada(derep_obj, err = reverse_errors, 
                                            multithread = FALSE, verbose = TRUE)})

```

# Merge paired reads
The denoised pairs of forward and reverse reads are merged if they are identical

```{r}
merged_reads = mergePairs(forward_dada, 
                          forward_derep, 
                          reverse_dada, 
                          reverse_derep, 
                          verbose = TRUE)
```


# Construct sequence table
Now the merged reads can be used to create a sequence table.  The size distribution
of the generated ASVs is assessed. 

If the sequencing was performed by *NCCT* it is likely of the V4 region and the
expected amplicon length is 250 base pairs. This should be reflected in the 
output of the following chunk, where most ASVs should have a length of ~250 bp.

If the sequencing was performed by *GeneWiz*, it is likely the V3-V4 regions were
sequenced, which result in a longer length because the overlap between the reads
is lower. The ASVs should have a length between ~390 and ~430 bp, with a bimodal
distribution, that is, high numbers around 400 and around 420.


```{r}
seq_table_raw = makeSequenceTable(merged_reads)
dim(seq_table_raw)
table(nchar(getSequences(seq_table_raw)))
```

If the size distribution is correct, we can then restrict the final set of
ASVs to the expected size rage. Therefore, values between 393 and 426 are
reasonable.

If you're running the workflow using *GeneWiz* sequencing, you most likely don't
need to modify the values.

```{r}
# Remove sequences with length above or below expected size
Min_len = 393
Max_len = 426

# TODO Check that the size range makes sense
Check_point("Was the amplicon size range adjusted accordingly?")

seq_table = seq_table_raw[,nchar(colnames(seq_table_raw)) %in% Min_len:Max_len]
dim(seq_table)
table(nchar(getSequences(seq_table)))
```


# Remove chimeras

Next, chimeras are removed. Chimeras are sequencing errors that arise when two 
unrelated strands merge. The next chunk outputs the fraction of reads that are 
*not* chimeras, that is, good ASVs that can be used for downstream analyses. 

In a normal sequencing run from *NCCT*, this should >90%.

In a run from *GeneWiz* the number of non-chimeric sequences expected is ~80%.
Chimera rates tend to be higher in multi-V-region because the conserved region
in the middle makes it easier for partial amplicons to re-anneal to others 
during PCR.

```{r}
seq_table_dechimered = removeBimeraDenovo(seq_table,
                                          method = "consensus", 
                                          multithread = FALSE, 
                                          verbose = TRUE)

# Proportion of non-chimeras

non_chimeras = round(sum(seq_table_dechimered)/sum(seq_table), 3)*100 

# Print
str_glue("The percentage of non-chimeric sequences was {nonchim}%",
         nonchim = non_chimeras)
```

# Number of reads through the pipeline

The following code generates a graph that illustrates how does the number of 
sequences changes along the processing workflow for each sample.
```{r}
getN = function(x) sum(getUniques(x))

Sequence_counts_workflow = filtered_trimmed %>% 
  as.data.frame() %>% 
  bind_cols(map_dbl(forward_dada, getN), 
            map_dbl(reverse_dada, getN),
            map_dbl(merged_reads, getN),
            rowSums(seq_table_dechimered)) %>% 
  rownames_to_column("Sample")

colnames(Sequence_counts_workflow) = c("Sample", "input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")


# Print
Sequence_counts_workflow
```

```{r, fig.height=4, fig.width=4}
Sequence_counts_workflow %>% 
  pivot_longer(cols = -Sample, names_to = "Step", values_to = "n_reads") %>% 
  mutate(Step = factor(Step, levels = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim"))) %>% 
  ggplot(aes(x = Step, y = n_reads, group = Sample, color = Sample)) +
    geom_point() +
    geom_line() +
    theme_light() +
    theme(legend.position = "none", axis.text.x = element_text(angle = 20, hjust = 1)) +
    labs(x = "Workflow Step", y = "Number of reads")
```


# Assign taxonomy

Once the clean ASVs are obtained, the taxonomic classification is assigned.
This is a general classification, it is not specific to any microbial community
in particular.

The more exact classification performed for samples derived from synthetic
communities is performed in a separate notebook.
```{r}
merged_taxonomy = assignTaxonomy(seq_table_dechimered, 
                       gtdb_tax,
                       minBoot = 70,
                       tryRC = TRUE,
                       multithread = FALSE)

merged_taxonomy_sp_raw = addSpecies(merged_taxonomy, 
                                    gtdb_species, 
                                    verbose = TRUE)
```

```{r}
# Data frame with taxonomy
# Add md5 sum for shorter unique ID of sequence
# Ad ASV#### identifier for ease of use
merged_taxonomy_sp_tmp = merged_taxonomy_sp_raw %>% 
  as.data.frame() %>% 
  rownames_to_column("Seq") %>% 
  bind_cols(md5 = map_chr(.$Seq, function(x) digest::digest(x, algo = "md5"))) %>% 
  mutate(ID = 1:nrow(.),
         ID = str_glue("ASV{id}", id = str_pad(ID, width = 4, pad = "0")), 
         ID = as.character(ID)) %>% 
  relocate(ID, md5)

tax_lvl = merged_taxonomy_sp_tmp  %>% 
  select(-c(ID, md5, Seq)) %>% 
  mutate_all(function(x) if_else(is.na(x), 0, 1)) %>% 
  rowSums()


# Add full name
# Add unclassified in case of missing level
merged_taxonomy_sp = merged_taxonomy_sp_tmp %>% 
  mutate(full_levels = tax_lvl) %>% 
  mutate(Name = case_when(full_levels == 1 ~ str_c("Unclassified", Kingdom, sep = " "), 
                          full_levels == 2 ~ str_c("Unclassified", Phylum, sep = " "),
                          full_levels == 3 ~ str_c("Unclassified", Class, sep = " "),
                          full_levels == 4 ~ str_c("Unclassified", Order, sep = " "),
                          full_levels == 5 ~ str_c("Unclassified", Family, sep = " "),
                          full_levels == 6 ~ str_c("Unclassified", Genus, sep = " "),
                          full_levels == 7 ~ str_c(Genus, Species, sep = " "))) %>% 
  relocate(ID, Name) %>% 
  select(-full_levels)



merged_taxonomy_sp %>% 
  head()
```

## Remove unclassified sequences

Finally, poorly classified ASVs are removed from the final table.
Such ASVs are usually the result of sequencing errors or, in the case of 
host-associated samples, these can originate from mitochondria or chloroplasts
from the host or its diet.

If you get a high proportion of poorly classified ASVs, this could be a sign that
something went wrong in the sequencing run.

In this case, sequences that don't have a phylum-level classifications will be
removed.

```{r}
# Identify poorly classified ASVs
Unclassified_ASVs = merged_taxonomy_sp %>% 
  filter(is.na(Phylum))

# Explore unclassified ASVs
Unclassified_ASVs 
```


```{r}
# Filter taxonomy table
merged_taxonomy_filt = merged_taxonomy_sp %>% 
  filter(!is.na(Phylum))
```

# Save files
Finally, the tables with the ASV abundances and taxonomic classification are 
generated and written. As with some of the steps above, you need to make sure
that the correct output directories and file names are specified


## Create data frames
```{r}
# Make sure sequence in column name corresponds to the taxonomy table
colnames(seq_table_dechimered) == merged_taxonomy_sp$Seq

# Create ASV table
# Replace names for ASV ID. Can also be changed to the md5
ASV_df_unfilt = seq_table_dechimered %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample") 

colnames(ASV_df_unfilt) = c("Sample", merged_taxonomy_sp$ID)

ASV_df = ASV_df_unfilt %>% 
  select(Sample, all_of(merged_taxonomy_filt$ID))

# Print
ASV_df %>% 
  head
```

## Write tables and tree to file
```{r}
# TODO Change the file names if needed
Check_point("Are the file names adjusted?")

# Give a name to your files
file_prefix = "Test_run"

out_asv_file = paste(file_prefix, "ASV_table.tsv", sep = "_") %>%
  file.path(out_dir, .)

out_tax_file = paste(file_prefix, "ASV_taxonomy.tsv", sep = "_") %>%
  file.path(out_dir, .)

write_tsv(ASV_df, out_asv_file)
write_tsv(merged_taxonomy_filt, out_tax_file)

# Optional
# Save workspace
# paste(file_prefix, "16S.RData.tsv", sep = "_") %>%
#   file.path(out_dir, .) %>%
#   save.image(.)
```

# (Optional) Construct phylogeny

Certain downstream analyses require a phylogeny of the members of the microbial
community, including Faith's phylogenetic diversity or the UniFrac family of
distances.

The following code was obtained from
https://compbiocore.github.io/metagenomics-workshop/assets/DADA2_tutorial.html

```{r}
# Retrieve sequences
ASV_seqs = getSequences(seq_table_dechimered)
names(ASV_seqs) = ASV_seqs
```

```{r}
# Align sequences
ASV_align = DECIPHER::AlignSeqs(Biostrings::DNAStringSet(ASV_seqs))

# Change sequence alignment output into a phyDat structure
ASV_align_matrix = phangorn::phyDat(as(ASV_align, "matrix"), type="DNA")

# Create distance matrix
ASV_dist = phangorn::dist.ml(ASV_align_matrix)

#Perform Neighbor joining

ASV_NJ_tree = phangorn::NJ(ASV_dist) # Note, tip order != sequence order

#Internal maximum likelihood
ASV_ML_fit = phangorn::pml(ASV_NJ_tree, data = ASV_align_matrix)

# negative edges length changed to 0!
ASV_ML_mod = update(ASV_ML_fit, k = 4, inv = 0.2)
ASV_ML_mod = phangorn::optim.pml(ASV_ML_mod, 
                         model = "GTR", 
                         optInv = TRUE, 
                         optGamma = TRUE,
                         rearrangement = "stochastic", 
                         control = phangorn::pml.control(trace = 0))
```


```{r}
# Save tree to object
ASV_ML_tree = ASV_ML_mod$tree
ASV_ML_tree %>% 
  class
```

```{r}
# Write tree file
out_tree_file = paste(file_prefix, "ASV_tree.tre", sep = "_") %>%
  file.path(out_dir, .)

ape::write.tree(ASV_ML_tree, file = out_tree_file)
```

