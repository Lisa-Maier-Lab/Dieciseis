---
title: "Taxonomic classify synthetic communities"
format: html
---

# Aim

Jacobo de la Cuesta-Zuluaga. December 2023.

This is the second notebook of the Maier Lab's 16S rRNA amplicon sequencing workflow.
*Note* that is only needed if you are processing the sequences of a synthetic community
such as EED or Com20. If you are processing complex samples, e.g. human feces, you
don't need to run this notebook.

In the last notebook we processed the raw sequences, genertated ASVs and performed
a general taxonomic classification. Here we will use full-length references to
classify the sequences of each member of the synthetic community and collapse the
abundance according to the taxonomic classification by species.

# Libraries

The following libraries are required for the present workflow

```{r}
library(tidyverse)
library(Biostrings)
library(DECIPHER)
library(ape)
library(conflicted)
```

```{r}
# Load helper functions
source("../bin/Helper_functions.R")
```

```{r}
# Solve conflics with certain function names
conflict_prefer("filter", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("rename", "dplyr")
conflict_prefer("setdiff", "base")
```

# Load files
```{r}
Check_point("Are the base and out directories filled?")
# TODO make sure the directories are correct
base_dir = "../test"

out_dir = file.path(base_dir, "dieciseis_out")
```

```{r}
# Load tables
# ASV table
ASV_df = file.path(out_dir, "Test_run_ASV_table.tsv") %>% 
  read_tsv()

# Taxonomy table
merged_taxonomy_sp = file.path(out_dir, "Test_run_ASV_taxonomy.tsv") %>% 
  read_tsv()
```


```{r}
Check_point("Is the path to the reference files correct?")
# Reference dbs
ref_dir = "../reference_files"

# Load reference files
# TODO make sure the corresponding taxonomy files are loaded
# This is only valid if using a defined community (e.g. Com20, EED)
SynthCom_species =  file.path(ref_dir, "COM20_GTDB_addSpecies.fna.gz")
SynthCom_ref_taxonomy = read_tsv(file.path(ref_dir, "COM20_GTDB_Taxonomy.txt")) %>%
  mutate(GTDB_Taxonomy = str_remove_all(GTDB_Taxonomy, "[a-z]__")) %>%
  separate(GTDB_Taxonomy, 
           into = c("Domain", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
           sep = ";")
```


# Classify 
```{r}
# Read fasta with full length 16S rRNA sequences of EED species
SynthCom_full = SynthCom_species %>% 
  readDNAStringSet() %>% 
  as.character()

ASV_sequences = merged_taxonomy_sp$Seq
names(ASV_sequences) = merged_taxonomy_sp$ID

# Combine full and amplicon sequences
# Convert to DNA stringset
combined_stringset = c(ASV_sequences, SynthCom_full) %>% 
  DNAStringSet()

# Align sequences
combined_alignment = AlignSeqs(combined_stringset, verbose = FALSE)

# Calculate pairwise distances
combined_distances = combined_alignment %>% 
  as.DNAbin() %>% 
  dist.dna(model = "raw")
```

```{r}
# Extract distance matrix
# Long df 
# Fix names
# Arrange by distance
combined_distances_long = combined_distances %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column("ASV_query") %>% 
  pivot_longer(cols = -ASV_query, names_to = "ASV_target", values_to = "Distance") %>%
  filter(str_detect(ASV_query, "ASV"), 
         !str_detect(ASV_target, "ASV")) %>% 
  mutate(ASV_target = word(ASV_target, start = 2, end = -1)) %>% 
  arrange(ASV_query, Distance)

# Filter to matches with 98 or higher identity
SynthCom_df_raw = combined_distances_long %>% 
  filter(Distance <= 0.02) %>% 
  group_by(ASV_query) %>% 
  slice(1) %>% 
  ungroup()

# Merge with taxonomy table and organize column order
SynthCom_df = SynthCom_df_raw  %>% 
  left_join(merged_taxonomy_sp, by = c("ASV_query" = "ID")) %>% 
  select(-Name) %>% 
  rename("Name" = "ASV_target", "ID" = "ASV_query") %>% 
  mutate(Species = word(Name, 2)) %>% 
  relocate(Distance, .after =  last_col()) %>% 
  arrange(ID)

SynthCom_df
```

# Collapse abundances
```{r}
# Taxonomy of each ASV from the synthetic community
SynthCom_taxonomy_collapse = SynthCom_df %>% 
  select(-c(md5, Seq)) %>% 
  distinct() %>% 
  arrange(Phylum, Class)

# Long format df with abundance and tax of each species on each sample
SynthCom_ASV_df = ASV_df %>% 
  pivot_longer(cols = -Sample, names_to = "ID", values_to = "Reads") %>% 
  left_join(SynthCom_taxonomy_collapse, by = join_by("ID"))

# Add up abundances of each species on each sample
SynthCom_colapsed = SynthCom_ASV_df %>% 
  group_by(Sample, Name) %>% 
  summarize(Sum_reads = sum(Reads)) %>% 
  pivot_wider(id_cols = Sample, names_from = Name, values_from = Sum_reads) %>% 
  ungroup()

# Print
SynthCom_colapsed %>% 
  head()
```
# Write tables
```{r}
# TODO Change the file names if needed
Check_point("Are the file names adjusted?")

# Give a name to your files
file_prefix = "Test_run"

out_collapsed_file = paste(file_prefix, "collapsed.tsv", sep = "_") %>% 
  file.path(out_dir, .)

out_synthcom_tax_file = paste(file_prefix, "exact_tax.tsv", sep = "_") %>% 
  file.path(out_dir, .)

write_tsv(SynthCom_colapsed, out_collapsed_file)
write_tsv(SynthCom_df, out_synthcom_tax_file)
```


