---
title: "Various 16S analysis - DADA2"
output: html_notebook
---


Jacobo de la Cuesta-Zuluaga. November 2023.

The aim of this notebook is to process the amplicon sequences from EED sample's using DADA2

For tutorials and explanations of the steps, see:
https://benjjneb.github.io/dada2/tutorial_1_8.html 
https://alexiscarter.github.io/metab/Dada_script_EN.html

# Libraries
```{r}
library(tidyverse)
library(dada2)
library(DECIPHER)
library(ape)
library(Biostrings)
library(conflicted)
```

```{r}
conflict_prefer("filter", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("rename", "dplyr")
conflict_prefer("setdiff", "base")
```


# Prompt
This is a function to remind the user to fill the corresponding fields!
```{r}
Check_point = function(Message){
  Variables_set = askYesNo(msg = Message)
  stopifnot("Make sure the fields are filled!" = isTRUE(Variables_set))
}

```

# Paths
```{r}
Check_point("Are the base and out directories filled?")
# TODO make sure the directories are correct
base_dir = "CHANGE ME!"

# TODO make sure the out directory is correct
out_dir = file.path(base_dir, "dada2_out")
dir.create(out_dir)

# Out dir
tables_dir = file.path(base_dir, "clean_tables")
dir.create(tables_dir)
```


```{r}
Check_point("Are the paths to the sequencing reads correct?")
# TODO make sure the sequence directories are correct
# Raw sequences
seqs_dir_base ="CHANGE ME!"
seqs_dir =  file.path(seqs_dir_base, "CHANGE ME!")

# Filtered sequences
filt_seqs_dir = file.path(out_dir, "EED_filtered_seqs")
dir.create(filt_seqs_dir)

# Reference dbs
ref_dir = "Q:/NAS/AG_Maier/Amplicon_Sequencing/Reference_files/"

gtdb_tax = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna.gz")
gtdb_species = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.addSpecies.fna.gz")


# TODO make sure the corresponding taxonomy files are loaded
# This is only valid if using a defined community (e.g. Com20, EED)
Check_point("Are the correct taxonomy files loaded?")
EED_species =  file.path(ref_dir, "STU25_GTDB_addSpecies.fna")
EED_ref_taxonomy = read_tsv(file.path(ref_dir, "STU25_GTDB_Taxonomy.txt")) %>% 
  select(GTDB_Name) %>% 
  separate(GTDB_Name, into = c("Genus", "Species"), sep = " ", remove = FALSE)
```

*Note* that I had to manually change the GTDB reference files.
They were origianlly obtained from https://scilifelab.figshare.com/articles/dataset/SBDI_Sativa_curated_16S_GTDB_database/14869077 (June 2022).
They have a double domain entry because the authors of these files require them in such way for their pipeline.
However, this leads to problems when used directly on dada2.
To fix, I used the following commands:

```
zcat gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fna.gz | sed -E 's~>Bacteria;|>Archaea;~>~' | sed '/>.*/s/$/;/' > gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
gzip gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
```

```{r}
# TODO make sure that the F and R read patterns are filled
Check_point("If the correct file name pattern for Forward and Reverse reads added?")

# List files in raw sequences dir
forward_reads = seqs_dir %>% 
  list.files(full.names = TRUE, pattern = "_1.fastp.fastq.gz") 
  
reverse_reads = seqs_dir %>%
  list.files(full.names = TRUE, pattern = "_2.fastp.fastq.gz")
  

# Make sure the corresponding F and R files are present
forward_names = str_remove(forward_reads, "_1.fastp.fastq.gz")
reverse_names = str_remove(reverse_reads, "_2.fastp.fastq.gz")

# TODO make sure that the file names make sense and that all files are paired
setdiff(forward_names, reverse_names)
setdiff(reverse_names, forward_names)
```

# Execute DADA2
## Check quality of data
```{r}
# TODO Add or remove random selection of reads for visual QC inspection

# Obtain quality plots of raw sequences for each of the files
Raw_QC_plotsF = sample(forward_reads, 10) %>%
  map(., function(fastq) {plotQualityProfile(fastq)})

Raw_QC_plotsR = sample(reverse_reads, 10) %>%
  map(., function(fastq) {plotQualityProfile(fastq)})
```

```{r}
# Print forward plots
Raw_QC_plotsF_hline = map(Raw_QC_plotsF, function(x) x + geom_hline(yintercept = 30))
Raw_QC_plotsF_hline[1:10]
```

```{r}
# Print Rreverse plots
Raw_QC_plotsR_hline = map(Raw_QC_plotsR, function(x) x + geom_hline(yintercept = 30))
Raw_QC_plotsR_hline[1:10]
```

## Determine positions to truncate
```{r}
# Function to calculate mean quality per position
Per_pos_mean = function(QC_df){
  QC_df %>% 
  group_by(Cycle) %>% 
  mutate(weight = Score * Count, 
         nbases = sum(Count)) %>% 
  summarize(mean_qual = sum(weight)/median(nbases)) %>% 
  ungroup()
}
```

```{r}
# Mean Quality DFs forward
mean_quals_F = map(Raw_QC_plotsF, function(qual_df){
  Per_pos_mean(qual_df$data)
})

# Mean Quality DFs reverse
mean_quals_R = map(Raw_QC_plotsR, function(qual_df){
  Per_pos_mean(qual_df$data)
})
```


```{r}
# # Determine on each sample first position with quality < 31
# first_drop_F = map_dbl(mean_quals_F, function(QC_df){
#   QC_df %>% 
#   filter(Cycle> 10, mean_qual < 34) %>% 
#   arrange(Cycle) %>% 
#   slice(1) %>% 
#   pull(Cycle)
# })
# 
# # Mean position with quality < 32
# first_drop_F %>% 
#   median()
# 
# # Determine on each sample first position with quality < 30
# first_drop_R = map_dbl(mean_quals_R, function(QC_df){
#   QC_df %>% 
#   filter(Cycle> 10, mean_qual < 35) %>% 
#   arrange(Cycle) %>% 
#   slice(1) %>% 
#   pull(Cycle)
# })
# 
# 
# first_drop_R %>% 
#   median()
```

Based on these results, combined with the plots above and the information on the DADA2 tutorial, for these data, I will use a truncation values of **225** for the forward reads, and **200** for the reverse reads.

# Filter reads
```{r}
# Filtering
filtered_trimmed = filterAndTrim(fwd = forward_reads, 
              rev = reverse_reads, 
              filt = filt_seqs_dir, 
              filt.rev = filt_seqs_dir,
              trimLeft = c(23,24), # Trim nucleotides corresponding to primer sequences. To check align primers to raw seqs
              truncLen=c(225, 200), # Truncation position of F and R
              maxEE=c(2,2), # Number of expected errors
              truncQ=11, # Truncate reads at the first instance of a quality score less than
              rm.phix=TRUE, # filter matches against the phiX
              compress=TRUE, 
              verbose=TRUE, 
              multithread=FALSE)
```

#  Learn errors
```{r}
# TODO make sure that the F and R read patterns are filled
Check_point("Are the correct file name pattern for Forward and Reverse reads added?")

# List files in raw sequences dir
forward_filt_reads = list.files(filt_seqs_dir, pattern = "_1.fastp.fastq.gz", full.names = TRUE)
reverse_filt_reads = list.files(filt_seqs_dir, pattern = "_2.fastp.fastq.gz", full.names = TRUE)

# Make sure the corresponding F and R files are present
forward_names = str_remove(basename(forward_filt_reads), "_1.fastp.fastq.gz")
reverse_names = str_remove(basename(reverse_filt_reads), "_2.fastp.fastq.gz")

setdiff(forward_names, reverse_names)
setdiff(reverse_names, forward_names)

# Add names to vectors
names(forward_filt_reads) = forward_names
names(reverse_filt_reads) = reverse_names
```

## Forward
```{r}
# Learn error rates
set.seed(2112)
forward_errors = learnErrors(forward_filt_reads, 
                             nbases = 1e8, 
                             randomize=TRUE,
                             multithread=FALSE, 
                             verbose = TRUE)

```

```{r}
plotErrors(forward_errors, nominalQ=TRUE)
```

## Reverse
```{r}
reverse_errors = learnErrors(forward_filt_reads, 
                             nbases = 1e8, 
                             randomize=TRUE,
                             multithread=FALSE, 
                             verbose = TRUE)
```

```{r}
plotErrors(reverse_errors, nominalQ=TRUE)
```

# Infer sequence variants
## Forward
```{r}
forward_derep = map(forward_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

forward_dada = map(forward_derep, 
                   function(derep_obj){dada(derep_obj, err=forward_errors, multithread=FALSE, verbose = TRUE)})
```

## Reverse
```{r}
reverse_derep = map(reverse_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

reverse_dada = map(reverse_derep, 
                   function(derep_obj){dada(derep_obj, err=reverse_errors, multithread=FALSE, verbose = TRUE)})

```

# Merge paired reads
```{r}
merged_reads = mergePairs(forward_dada, 
                          forward_derep, 
                          reverse_dada, 
                          reverse_derep, 
                          verbose=TRUE)
```


# Construct sequence table
```{r}
seq_table_raw = makeSequenceTable(merged_reads)
dim(seq_table_raw)
table(nchar(getSequences(seq_table_raw)))
```

```{r}
# TODO Check that the size range makes sense
# Remove sequences with length above or below expected size
seq_table = seq_table_raw[,nchar(colnames(seq_table_raw)) %in% 250:254]
dim(seq_table)
table(nchar(getSequences(seq_table)))


Check_point("Was the amplicon size range adjusted accordingly?")
```


# Remove chimeras
```{r}
seq_table_dechimered <- removeBimeraDenovo(seq_table,
                                    method="consensus", 
                                    multithread=FALSE, 
                                    verbose=TRUE)
dim(seq_table_dechimered)

# Proportion of non-chimeras
sum(seq_table_dechimered)/sum(seq_table)
```

# Number of reads through the pipeline
```{r}
getN = function(x) sum(getUniques(x))


Sequence_counts_workflow = filtered_trimmed %>% 
  as.data.frame() %>% 
  bind_cols(map_dbl(forward_dada, getN), 
            map_dbl(reverse_dada, getN),
            map_dbl(merged_reads, getN),
            rowSums(seq_table_dechimered)) %>% 
  rownames_to_column("Sample")

colnames(Sequence_counts_workflow) = c("Sample", "input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")


# Print
Sequence_counts_workflow
```

```{r, fig.height=8, fig.width=7}
Sequence_counts_workflow %>% 
  pivot_longer(cols = -Sample, names_to = "Step", values_to = "n_reads") %>% 
  mutate(Step = factor(Step, levels = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim"))) %>% 
  ggplot(aes(x = Step, y = n_reads, group = Sample, color = Sample)) +
    geom_point() +
    geom_line() +
    theme_light() +
    theme(legend.position = "none") +
    labs(x = "Workflow Step", y = "Number of reads")
```


# Assign taxonomy
```{r}
merged_taxonomy = assignTaxonomy(seq_table_dechimered, 
                       gtdb_tax,
                       minBoot=70,
                       tryRC = TRUE,
                       multithread=FALSE)

merged_taxonomy_sp_raw = addSpecies(merged_taxonomy, 
                                    gtdb_species, 
                                    verbose=TRUE)

unname(merged_taxonomy_sp_raw) %>% 
  as.data.frame()
```

```{r}
# Data frame with taxonomy
# Add md5 sum for shorter unique ID of sequence
# Ad ASV#### identifier for ease of use
merged_taxonomy_sp_tmp = merged_taxonomy_sp_raw %>% 
  as.data.frame() %>% 
  rownames_to_column("Seq") %>% 
  bind_cols(md5 = map_chr(.$Seq, function(x) digest::digest(x, algo = "md5"))) %>% 
  mutate(ID = 1:nrow(.),
         ID = str_glue("ASV{id}", id = str_pad(ID, width = 4, pad = "0")), 
         ID = as.character(ID)) %>% 
  relocate(ID, md5)

tax_lvl = merged_taxonomy_sp_tmp  %>% 
  select(-c(ID, md5, Seq)) %>% 
  mutate_all(function(x) if_else(is.na(x), 0, 1)) %>% 
  rowSums()


# Add full name
# Add unclassified in case of missing level
merged_taxonomy_sp = merged_taxonomy_sp_tmp %>% 
  mutate(full_levels = tax_lvl) %>% 
  mutate(Name = case_when(full_levels == 1 ~ str_c("Unclassified", Kingdom, sep = " "), 
                          full_levels == 2 ~ str_c("Unclassified", Phylum, sep = " "),
                          full_levels == 3 ~ str_c("Unclassified", Class, sep = " "),
                          full_levels == 4 ~ str_c("Unclassified", Order, sep = " "),
                          full_levels == 5 ~ str_c("Unclassified", Family, sep = " "),
                          full_levels == 6 ~ str_c("Unclassified", Genus, sep = " "),
                          full_levels == 7 ~ str_c(Genus, Species, sep = " "))) %>% 
  relocate(ID, Name) %>% 
  select(-full_levels)



merged_taxonomy_sp %>% 
  head()
```


## Identify ASVs from EED
```{r}
# Exact matches using DADA2 functions
EED_exact_raw = addSpecies(merged_taxonomy, 
                         EED_species, 
                         verbose=TRUE)

# Data frame with taxonomy
# Add md5 sum for shorter unique ID of sequence
# Ad ASV#### identifier for ease of use
EED_exact = EED_exact_raw %>% 
  as.data.frame() %>% 
  rownames_to_column("Seq") %>% 
  left_join(select(merged_taxonomy_sp, ID, md5, Seq)) %>% 
  filter(!is.na(Species)) %>% 
  mutate(Distance = 0, Name = str_c(Genus, Species, sep = " ")) %>% 
  relocate(ID, Name, md5)


EED_exact
```

```{r}
# Read fasta with full length 16S rRNA sequences of EED species
EED_full = EED_species %>% 
  readDNAStringSet() %>% 
  as.character()

# Extract 16S sequence of potential EED ASVs
# Restrict to ASVs from the expected genera
potential_EED = merged_taxonomy_sp %>% 
  dplyr::filter(Genus %in% EED_ref_taxonomy$Genus,
                !(ID %in% EED_exact$ID))

# Add ASV ID
potential_ASVs = potential_EED$Seq 
names(potential_ASVs) = potential_EED$ID
```


```{r}
# Combine full and amplicon sequences
# Convert to DNA stringset
combined_stringset = c(potential_ASVs, EED_full) %>% 
  DNAStringSet()

# Align sequences
combined_alignment = AlignSeqs(combined_stringset, verbose = FALSE)

# Calculate pairwise distances
combined_distances = combined_alignment %>% 
  as.DNAbin() %>% 
  dist.dna(model = "raw")
```


```{r}
# Extract distance matrix
# Long df 
# Fix names
# Arrange by distance
combined_distances_long = combined_distances %>% 
  as.matrix() %>% 
  as.data.frame() %>% 
  rownames_to_column("ASV_query") %>% 
  pivot_longer(cols = -ASV_query, names_to = "ASV_target", values_to = "Distance") %>%
  filter(str_detect(ASV_query, "ASV"), 
         !str_detect(ASV_target, "ASV")) %>% 
  mutate(ASV_target = word(ASV_target, start = 2, end = -1)) %>% 
  arrange(ASV_query, Distance)

# Filter to matches with 98 or higher identity
EED_df_raw = combined_distances_long %>% 
  filter(Distance <= 0.02) %>% 
  group_by(ASV_query) %>% 
  slice(1) %>% 
  ungroup()

# Merge with taxonomy table and organize column order
EED_df = EED_df_raw  %>% 
  left_join(merged_taxonomy_sp, by = c("ASV_query" = "ID")) %>% 
  select(-Name) %>% 
  rename("Name" = "ASV_target", "ID" = "ASV_query") %>% 
  mutate(Species = word(Name, 2)) %>% 
  relocate(Distance, .after =  last_col()) %>% 
  bind_rows(., EED_exact) %>% 
  arrange(ID)

EED_df
```


# Construct phylogeny
From https://compbiocore.github.io/metagenomics-workshop/assets/DADA2_tutorial.html
```{r}
# Retrieve sequences
ASV_seqs = getSequences(seq_table_dechimered)
names(ASV_seqs) = ASV_seqs
```

```{r}
# Align sequences
ASV_align = DECIPHER::AlignSeqs(Biostrings::DNAStringSet(ASV_seqs))

# Change sequence alignment output into a phyDat structure
ASV_align_matrix = phangorn::phyDat(as(ASV_align, "matrix"), type="DNA")

# Create distance matrix
ASV_dist = phangorn::dist.ml(ASV_align_matrix)

#Perform Neighbor joining

ASV_NJ_tree = phangorn::NJ(ASV_dist) # Note, tip order != sequence order

#Internal maximum likelihood
ASV_ML_fit = phangorn::pml(ASV_NJ_tree, data = ASV_align_matrix)

# negative edges length changed to 0!
ASV_ML_mod = update(ASV_ML_fit, k=4, inv=0.2)
ASV_ML_mod = phangorn::optim.pml(ASV_ML_mod, 
                         model="GTR", 
                         optInv=TRUE, 
                         optGamma=TRUE,
                         rearrangement = "stochastic", 
                         control = phangorn::pml.control(trace = 0))
```


```{r}
# Save tree to object
ASV_ML_tree = ASV_ML_mod$tree
ASV_ML_tree %>% 
  class
```


# Save files
## Create data frames
```{r}
# Make sure sequence in column name corresponds to the taxonomy table
colnames(seq_table_dechimered) == merged_taxonomy_sp$Seq

# Create ASV table
# Replace names for ASV ID. Can also be changed to the md5
ASV_df = seq_table_dechimered %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample")

colnames(ASV_df) = c("Sample", merged_taxonomy_sp$ID)

# Print
ASV_df %>% 
  head
```

## Write tables and tree to file
```{r}
# TODO Change the file names if needed
Check_point("Are the file names adjusted?")

out_asv_file = file.path(out_dir, "2023_11_EED_ASV_table.tsv")
out_tax_file = file.path(out_dir, "2023_11_EED_taxonomy_full.tsv")
out_EED_tax_file = file.path(out_dir, "2023_11_EED_taxonomy_table.tsv")
out_tree_file = file.path(out_dir, "2023_11_EED_tree.tre")

write_tsv(ASV_df, out_asv_file)
write_tsv(merged_taxonomy_sp, out_tax_file)
write_tsv(EED_df, out_EED_tax_file)
saveRDS(seq_table_dechimered, file.path(out_dir, "2023_11_EED_ASVs.rds"))
saveRDS(merged_taxonomy_sp, file.path(out_dir, "2023_11_EED_taxonomy.rds"))
ape::write.tree(ASV_ML_tree, file = out_tree_file)
save.image(file.path(out_dir, "2023_11_EED_16S.RData"))
```

# Session Info
```{r}
sessionInfo()
```


